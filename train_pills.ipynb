{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of cambridge_hack.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVV0jlTtEUwo",
        "colab_type": "code",
        "outputId": "1081262a-9f1b-4ab3-c5fc-589ad208089e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "!wget https://github.com/git-lfs/git-lfs/releases/download/v2.6.1/git-lfs-linux-amd64-v2.6.1.tar.gz\n",
        "!tar -xzvf git-lfs-linux-amd64-v2.6.1.tar.gz\n",
        "!./install.sh\n",
        "!git lfs install\n",
        "!git lfs get"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-21 01:56:42--  https://github.com/git-lfs/git-lfs/releases/download/v2.6.1/git-lfs-linux-amd64-v2.6.1.tar.gz\n",
            "Resolving github.com (github.com)... 13.229.188.59\n",
            "Connecting to github.com (github.com)|13.229.188.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/13021798/aa276700-f730-11e8-96fb-bb179337f35f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190521%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190521T015642Z&X-Amz-Expires=300&X-Amz-Signature=d5f1a2c855eb87f32b7a82edb8295d77a4f7e70eb57f78482957bf6e32bba6d2&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dgit-lfs-linux-amd64-v2.6.1.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
            "--2019-05-21 01:56:42--  https://github-production-release-asset-2e65be.s3.amazonaws.com/13021798/aa276700-f730-11e8-96fb-bb179337f35f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20190521%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20190521T015642Z&X-Amz-Expires=300&X-Amz-Signature=d5f1a2c855eb87f32b7a82edb8295d77a4f7e70eb57f78482957bf6e32bba6d2&X-Amz-SignedHeaders=host&actor_id=0&response-content-disposition=attachment%3B%20filename%3Dgit-lfs-linux-amd64-v2.6.1.tar.gz&response-content-type=application%2Foctet-stream\n",
            "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.216.110.203\n",
            "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.216.110.203|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3610515 (3.4M) [application/octet-stream]\n",
            "Saving to: ‘git-lfs-linux-amd64-v2.6.1.tar.gz’\n",
            "\n",
            "git-lfs-linux-amd64 100%[===================>]   3.44M  2.00MB/s    in 1.7s    \n",
            "\n",
            "2019-05-21 01:56:45 (2.00 MB/s) - ‘git-lfs-linux-amd64-v2.6.1.tar.gz’ saved [3610515/3610515]\n",
            "\n",
            "README.md\n",
            "CHANGELOG.md\n",
            "git-lfs\n",
            "install.sh\n",
            "Git LFS initialized.\n",
            "Git LFS initialized.\n",
            "Error: unknown command \"get\" for \"git-lfs\"\n",
            "Run 'git-lfs --help' for usage.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRkCTyvoDRl9",
        "colab_type": "code",
        "outputId": "ffb44d43-911e-4019-9360-2afcdd6e987e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "import os\n",
        "# !git clone https://sadikneipp:@github.com/sadikneipp/hack-cambridge4D.git\n",
        "!git clone https://github.com/sadikneipp/pill-it-up.git\n",
        "os.chdir('pill-it-up')\n",
        "!git checkout cloud\n",
        "!git pull"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pill-it-up'...\n",
            "remote: Enumerating objects: 307, done.\u001b[K\n",
            "remote: Counting objects: 100% (307/307), done.\u001b[K\n",
            "remote: Compressing objects: 100% (307/307), done.\u001b[K\n",
            "remote: Total 5572 (delta 0), reused 304 (delta 0), pack-reused 5265\u001b[K\n",
            "Receiving objects: 100% (5572/5572), 238.72 MiB | 11.41 MiB/s, done.\n",
            "Resolving deltas: 100% (81/81), done.\n",
            "Checking out files: 100% (3393/3393), done.\n",
            "Filtering content: 100% (3380/3380), 83.63 MiB | 531.00 KiB/s, done.\n",
            "Branch 'cloud' set up to track remote branch 'cloud' from 'origin'.\n",
            "Switched to a new branch 'cloud'\n",
            "Already up to date.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ33-mWTkqGT",
        "colab_type": "code",
        "outputId": "eda29a0a-e09e-44da-8327-ac8c9d5311b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "#torch dependencies\n",
        "from os.path import exists\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "!pip install torch torchvision \n",
        "!pip install Pillow==4.0.0\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.2.2.post3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (4.3.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision) (0.46)\n",
            "Collecting Pillow==4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/37/e8/b3fbf87b0188d22246678f8cd61e23e31caa1769ebc06f1664e2e5fe8a17/Pillow-4.0.0-cp36-cp36m-manylinux1_x86_64.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow==4.0.0) (0.46)\n",
            "\u001b[31mERROR: torchvision 0.2.2.post3 has requirement pillow>=4.1.1, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: scikit-image 0.14.2 has requirement pillow>=4.3.0, but you'll have pillow 4.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: Pillow\n",
            "  Found existing installation: Pillow 4.3.0\n",
            "    Uninstalling Pillow-4.3.0:\n",
            "      Successfully uninstalled Pillow-4.3.0\n",
            "Successfully installed Pillow-4.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E65TNLvs_DKJ",
        "colab_type": "code",
        "outputId": "51c7bd31-d591-477e-c8c7-e9dc8b86561d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e32x3S0W-ltx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "os.chdir('model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I68nhf3_s-x",
        "colab_type": "code",
        "outputId": "08805b01-77e5-4ebc-f0fe-abf07ca6769b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models, transforms\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import random_split\n",
        "import copy\n",
        "from torch import nn\n",
        "import time\n",
        "import argparse\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#Environment Parameters\n",
        "COLAB = True\n",
        "MODEL_PATH = '../../gdrive/My Drive/demo_diretor/'\n",
        "\n",
        "BS = 8\n",
        "WORKERS= 2\n",
        "N_SPLITS = 1 # number of times to split dataset randomly and fit\n",
        "SPLIT_MODE = 'auto' #auto will use the torch dataset random splitter, manual will use the premade folder splits\n",
        "VAL_SIZE = 0.3 #percentage of validation data\n",
        "N_CLASSES = 5\n",
        "\n",
        "''' \n",
        "Training parameters \n",
        "\n",
        "Follows pytorch naming conventions   \n",
        "'''\n",
        "\n",
        "LR_0 = 0.01 #initial learning rate\n",
        "GAMMA = 0.1\n",
        "N_EPOCHS = 3 #total epochs\n",
        "STEP_SIZE = 1 #epochs until decay of lr by gamma\n",
        "MOMENTUM = 0.9\n",
        "WD = 1e-4 #weight decay\n",
        "\n",
        "def get_args(): #deprecated for colab e.e\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"-d\", \"--device\", help=\"pass 'gpu' to turn on GPU\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "def check_create(path):\n",
        "    if os.path.exists(path):\n",
        "        print(path + ' exists')\n",
        "        return\n",
        "        \n",
        "    os.makedirs(path)\n",
        "    print(path + ' created')\n",
        "\n",
        "def save_data(data):\n",
        "    check_create(OUTPUT_PATH)\n",
        "    now = str(int(time.time()))\n",
        "    with open(OUTPUT_PATH + now + '.pickle', 'wb') as handle:\n",
        "        pickle.dump(data, handle)\n",
        "\n",
        "def balanced_valid_ixs(valid):\n",
        "    crackle_ix = []\n",
        "    no_crackle_ix = []\n",
        "    \n",
        "    for i in range(len(valid)):\n",
        "        if valid[i]['label'].item() == 1:\n",
        "            crackle_ix.append(i)\n",
        "        else:\n",
        "            no_crackle_ix.append(i)\n",
        "    \n",
        "    return crackle_ix + no_crackle_ix[:len(crackle_ix)]\n",
        "\n",
        "def init_loaders():\n",
        "    tfs = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomVerticalFlip(),\n",
        "        transforms.RandomRotation(5),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "    if SPLIT_MODE == 'auto':\n",
        "        whole = datasets.ImageFolder('../acquisition/dataset_demo_diretor', transform=tfs)\n",
        "        n_train = int(len(whole)*(1-VAL_SIZE))\n",
        "        n_val = len(whole) - n_train\n",
        "        train, valid = random_split(whole, [n_train, n_val])\n",
        "\n",
        "    if SPLIT_MODE == 'manual':\n",
        "        train = datasets.ImageFolder('../acquisition/dataset_demo_diretor/train', transform=tfs)\n",
        "        valid = datasets.ImageFolder('../acquisition/dataset_demo_diretor/test', transform=tfs)\n",
        "        \n",
        "    trainloader = DataLoader(train, batch_size=BS, drop_last=False)\n",
        "\n",
        "    validloader = DataLoader(valid, batch_size=BS, drop_last=False,\n",
        "                            shuffle=False)\n",
        "\n",
        "    return trainloader, validloader\n",
        "\n",
        "def metrics(targets, y_pred):\n",
        "    return torch.sum(targets == y_pred).item() / targets.numel(), confusion_matrix(targets.numpy()\n",
        "                                                        , y_pred.numpy())\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "    upsample = nn.Upsample((224, 224))\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    # data = {'train_loss': [], 'train_acc': [], 'train_cnf': [],\n",
        "    #         'val_loss': [], 'val_acc': [], 'val_cnf': [],\n",
        "    #         'params': {\n",
        "    #             'LR_0': LR_0,\n",
        "    #             'GAMMA': GAMMA,\n",
        "    #             'N_EPOCHS': N_EPOCHS,\n",
        "    #             'STEP_SIZE': STEP_SIZE,\n",
        "    #             'MOMENTUM': MOMENTUM,\n",
        "    #             'WD': WD,\n",
        "    #             'CLASS_WEIGHTS': CLASS_WEIGHTS\n",
        "    #              }\n",
        "    #        }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            preds_acc = []\n",
        "            labels_acc = []\n",
        "\n",
        "            # Iterate over data.\n",
        "            for i, batch in enumerate(dataloaders[phase]):\n",
        "                inputs = batch[0]\n",
        "                labels = batch[1]\n",
        "                inputs = inputs.to(device, dtype=torch.float)\n",
        "                inputs = upsample(inputs)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statisticsstatisticsstatistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                preds_acc.append(preds)\n",
        "                labels_acc.append(labels)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc, cnf_mat = metrics(torch.cat(labels_acc).cpu(), torch.cat(preds_acc).cpu())\n",
        "            \n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            print(cnf_mat)\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") #some arcane cuda magic on my pc and this breaks\n",
        "\n",
        "if COLAB:\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    print('running on gpu! cuda:0')\n",
        "\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('running on cpu!')\n",
        "\n",
        "if SPLIT_MODE == 'manual': n_runs = 1 # if using a premade split, only run once\n",
        "else: n_runs = N_SPLITS\n",
        "\n",
        "for run in range(n_runs):\n",
        "\n",
        "    trainloader, validloader = init_loaders()\n",
        "    print(f'Run {run+1} out of {n_runs}')\n",
        "    dataloaders = {'train': trainloader, 'val': validloader}\n",
        "    dataset_sizes = {x: len(dataloaders[x])*BS for x in ['train', 'val']}\n",
        "\n",
        "    model_ft = models.resnet18(pretrained=True)\n",
        "    num_ftrs = model_ft.fc.in_features\n",
        "    model_ft.fc = nn.Linear(num_ftrs, N_CLASSES)\n",
        "\n",
        "    model_ft = model_ft.to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Observe that all parameters are being optimized\n",
        "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=LR_0, momentum=MOMENTUM, weight_decay = WD)\n",
        "#     optimizer_ft = optim.RMSprop(model_ft.parameters(), lr=LR_0, momentum=MOMENTUM, weight_decay = WD)\n",
        "#     optimizer_ft = optim.Adam(model_ft.parameters(), lr=LR_0, weight_decay = WD)\n",
        "    \n",
        "    \n",
        "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=STEP_SIZE, gamma=GAMMA)\n",
        "    model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                        num_epochs=N_EPOCHS)\n",
        "\n",
        "    now = str(int(time.time()))\n",
        "    torch.save(model_ft.state_dict(), MODEL_PATH + now + '.model')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "running on gpu! cuda:0\n",
            "Run 1 out of 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n",
            "100%|██████████| 46827520/46827520 [00:00<00:00, 107441494.86it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/2\n",
            "----------\n",
            "train Loss: 0.6609 Acc: 0.7464\n",
            "[[ 91   4  23   2   0]\n",
            " [  4  54   9   0   2]\n",
            " [ 26   2 118  10   1]\n",
            " [  6   0  18  44   0]\n",
            " [  6   1  10   0  58]]\n",
            "val Loss: 0.0578 Acc: 0.9953\n",
            "[[60  0  0  0  0]\n",
            " [ 0 31  0  0  0]\n",
            " [ 1  0 62  0  0]\n",
            " [ 0  0  0 32  0]\n",
            " [ 0  0  0  0 25]]\n",
            "\n",
            "Epoch 1/2\n",
            "----------\n",
            "train Loss: 0.1442 Acc: 0.9673\n",
            "[[116   0   4   0   0]\n",
            " [  0  69   0   0   0]\n",
            " [  8   0 147   0   2]\n",
            " [  0   0   1  67   0]\n",
            " [  0   0   1   0  74]]\n",
            "val Loss: 0.0176 Acc: 1.0000\n",
            "[[60  0  0  0  0]\n",
            " [ 0 31  0  0  0]\n",
            " [ 0  0 63  0  0]\n",
            " [ 0  0  0 32  0]\n",
            " [ 0  0  0  0 25]]\n",
            "\n",
            "Epoch 2/2\n",
            "----------\n",
            "train Loss: 0.1242 Acc: 0.9816\n",
            "[[115   0   5   0   0]\n",
            " [  0  69   0   0   0]\n",
            " [  1   0 154   2   0]\n",
            " [  0   0   0  68   0]\n",
            " [  0   0   1   0  74]]\n",
            "val Loss: 0.0199 Acc: 1.0000\n",
            "[[60  0  0  0  0]\n",
            " [ 0 31  0  0  0]\n",
            " [ 0  0 63  0  0]\n",
            " [ 0  0  0 32  0]\n",
            " [ 0  0  0  0 25]]\n",
            "\n",
            "Training complete in 0m 32s\n",
            "Best val Acc: 1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbf1SxX6EcLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}